# -*- coding: utf-8 -*-
"""logistic regression titanic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G-wlpt1F19rOVICRMngPhunvNPpx7KdU
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#import titanic dataset
titanic = pd.read_csv('titanic_train.csv')
titanic.head()

#count number of passenger who survived and Not survived : 0 = not survived,1 = survived
sns.countplot(x='Survived',data=titanic)
plt.show()

#count number of males and females who survived and Not survived : 0 = not survived,1 = survived
sns.countplot(x='Survived',hue='Sex',data=titanic)
plt.show()

#count number of passenger class who survived the most
sns.barplot(x='Pclass',y='Survived',hue="Sex",data=titanic)
plt.show()

#verify missing values  in each column
titanic.isnull().sum()

#check data types of each column and hence finding out which columns are categorical in nature
titanic.info()

# function to impute missing values in age columns based on class
def impute_age(cols):
    Age = cols[0]
    Pclass = cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 10
        elif Pclass == 2:
            return 20
        else:
            return 24
    else:
        return Age

# impute missing values in age clumn by average age
titanic['Age'] = titanic[['Age','Pclass']].apply(impute_age,axis=1)

titanic.head()

# drop cabin column
titanic.drop('Cabin',inplace=True,axis=1)

titanic.head()

#inpute Enbarked missing values with most common values
most_common_value = 'S'

titanic['Embarked'] = titanic['Embarked'].fillna(most_common_value)

#check if there are any missing values left:
titanic.isnull().sum()

titanic['Sex'].head()

titanic['Pclass'].head()

titanic['Embarked'].head()

"""# always drop the first column after performing One Hot Encoding?

since one of the column can be generated completely frrom the others, and hence relating this extra columns does not add any new information for the modelling process, would it be good practice to always drop the first column after performing One Hot Encoding regardless of the algorithms of choice.
"""

#convert categorical variables into dummy variables
sex = pd.get_dummies(titanic['Sex'],drop_first=True)
pclass = pd.get_dummies(titanic['Pclass'],drop_first=True)
embark = pd.get_dummies(titanic['Embarked'],drop_first=True)

sex.head()

pclass.head()

embark.head()

# drop irrelevant columns
titanic.drop(['PassengerId','Sex','Embarked','Name','Ticket','Pclass','Age'],axis=1,inplace= True)

#validate the column and data
titanic.head()

#concatenate the dummy variables, create above
titanic = pd.concat([titanic,sex,embark,pclass],axis=1)
titanic.head()

#create feature variables X and Target variable y
x= titanic.drop('Survived',axis=1)
y= titanic['Survived']

x.head()

y.head()

#split the data into training set (70% )and test set(30%)
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=30,random_state = 10)

# fit the logistic regression model
from sklearn.linear_model import LogisticRegression
logmodel = LogisticRegression(C=100,random_state=90)
logmodel.fit(x_train,y_train)

#predict the value for new, unseen data
y_pred = logmodel.predict(x_test)
y_pred

print('wrong prediction out of total:')
print((y_test!=y_pred).sum(),'/',((y_test==y_pred).sum()+(y_test!=y_pred).sum()))

from sklearn.metrics import accuracy_score
print('Percentage Accuracy:',100*accuracy_score(y_test,y_pred))

#confusion matrics
from sklearn.metrics import confusion_matrix
c_m = confusion_matrix(y_test,y_pred)
c_m

#how to find out missclassified samples

import numpy as np
np.where(y_pred!=y_test)

